# -*- coding: utf-8 -*-
"""ml

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cb7_OM68WK71EP75roncqDoT_F4WAY9u
"""



"""Complete Code

"""

import pandas as pd
import torch
import streamlit as st
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.metrics import precision_recall_fscore_support
from nltk.translate.bleu_score import sentence_bleu

# Import Transformer Model and Tokenizer
from transformers import RobertaTokenizer, T5ForConditionalGeneration

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load dataset
try:
    df = pd.read_csv("/content/synthetic_bug_dataset_1000_final_refreshed.csv").dropna()
except FileNotFoundError:
    print("Error: File not found. Please provide the correct file path.")
    raise  # Stop execution if file is not found

# Preprocess Data
def preprocess_data(df):
    if df.empty:
        print("DataFrame is empty. Please load the dataset first.")
        return [], []

    # Find column names containing 'buggy' and 'fixed'
    buggy_column_name = df.columns[df.columns.str.contains('buggy', case=False)][0]
    fixed_column_name = df.columns[df.columns.str.contains('fixed', case=False)][0]

    return df[buggy_column_name].tolist(), df[fixed_column_name].tolist()

# Load Pretrained Model
MODEL_NAME = "Salesforce/codet5-small"
tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)

# Custom Dataset
class CodeDataset(Dataset):
    def __init__(self, buggy, fixed, tokenizer, max_length=512):
        self.buggy = buggy
        self.fixed = fixed
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.buggy)

    def __getitem__(self, idx):
        buggy_code = "fix: " + self.buggy[idx]
        fixed_code = self.fixed[idx]

        source = self.tokenizer(buggy_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")
        target = self.tokenizer(fixed_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")

        return {
            "input_ids": source["input_ids"].squeeze(),
            "attention_mask": source["attention_mask"].squeeze(),
            "labels": target["input_ids"].squeeze()
        }

# Train-Test Split
buggy, fixed = preprocess_data(df)
train_buggy, val_buggy, train_fixed, val_fixed = train_test_split(buggy, fixed, test_size=0.2, random_state=42)

# Create Datasets & Dataloaders
train_dataset = CodeDataset(train_buggy, train_fixed, tokenizer)
val_dataset = CodeDataset(val_buggy, val_fixed, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training Function
def train_model(model, train_loader, val_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        loop = tqdm(train_loader, leave=True)
        for batch in loop:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            loop.set_description(f"Epoch {epoch+1}")
            loop.set_postfix(loss=loss.item())

        # Validation Step
        model.eval()
        with torch.no_grad():
            val_loss = 0
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()
        print(f"Validation Loss after Epoch {epoch+1}: {val_loss/len(val_loader):.4f}")
        model.train()

# Train the Model
train_model(model, train_loader, val_loader, epochs=3)

# Save Model
model.save_pretrained("bug_fixing_t5_model")
tokenizer.save_pretrained("bug_fixing_t5_model")

# Load trained model for inference
def load_model():
    model = T5ForConditionalGeneration.from_pretrained("bug_fixing_t5_model").to(device)
    tokenizer = RobertaTokenizer.from_pretrained("bug_fixing_t5_model")  # FIXED
    return model, tokenizer

model, tokenizer = load_model()

# Code Fixing Function
def fix_code(buggy_code):
    """Generate a fixed version of the buggy code"""
    input_text = "fix: " + buggy_code
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)
    output_ids = model.generate(input_ids, max_length=512)
    fixed_code = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return fixed_code

# Evaluation Function
def evaluate_model(val_buggy, val_fixed):
    predictions = [fix_code(code) for code in val_buggy]

    precision, recall, f1, _ = precision_recall_fscore_support(
        [fixed.split() for fixed in val_fixed],
        [pred.split() for pred in predictions],
        average='macro'
    )

    bleu_scores = [sentence_bleu([fixed.split()], pred.split()) for fixed, pred in zip(val_fixed, predictions)]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, BLEU Score: {avg_bleu:.4f}")

evaluate_model(val_buggy, val_fixed)

# Streamlit UI
if __name__ == "__main__":
    st.title("Bug Detection & Auto Fixing System")
    user_input = st.text_area("Enter buggy code:")
    if st.button("Fix Code"):
        if user_input.strip():
            fixed_output = fix_code(user_input)
            st.subheader("Fixed Code:")
            st.code(fixed_output, language='python')
        else:
            st.warning("Please enter some code to fix.")

import pandas as pd
import torch
!pip install streamlit
import streamlit as st
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.metrics import precision_recall_fscore_support
from nltk.translate.bleu_score import sentence_bleu

# Import Transformer Model and Tokenizer
from transformers import RobertaTokenizer, T5ForConditionalGeneration

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load dataset
try:
    df = pd.read_csv("/content/synthetic_bug_dataset_1000_final_refreshed.csv").dropna()
except FileNotFoundError:
    print("Error: File not found. Please provide the correct file path.")
    raise  # Stop execution if file is not found

# Preprocess Data
def preprocess_data(df):
    if df.empty:
        print("DataFrame is empty. Please load the dataset first.")
        return [], []

    # Find column names containing 'buggy' and 'fixed'
    buggy_column_name = df.columns[df.columns.str.contains('buggy', case=False)][0]
    fixed_column_name = df.columns[df.columns.str.contains('fixed', case=False)][0]

    return df[buggy_column_name].tolist(), df[fixed_column_name].tolist()

# Load Pretrained Model
MODEL_NAME = "Salesforce/codet5-small"
tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)

# Custom Dataset
class CodeDataset(Dataset):
    def __init__(self, buggy, fixed, tokenizer, max_length=512):
        self.buggy = buggy
        self.fixed = fixed
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.buggy)

    def __getitem__(self, idx):
        buggy_code = "fix: " + self.buggy[idx]
        fixed_code = self.fixed[idx]

        source = self.tokenizer(buggy_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")
        target = self.tokenizer(fixed_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")

        return {
            "input_ids": source["input_ids"].squeeze(),
            "attention_mask": source["attention_mask"].squeeze(),
            "labels": target["input_ids"].squeeze()
        }

# Train-Test Split
buggy, fixed = preprocess_data(df)
train_buggy, val_buggy, train_fixed, val_fixed = train_test_split(buggy, fixed, test_size=0.2, random_state=42)

# Create Datasets & Dataloaders
train_dataset = CodeDataset(train_buggy, train_fixed, tokenizer)
val_dataset = CodeDataset(val_buggy, val_fixed, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training Function
def train_model(model, train_loader, val_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        loop = tqdm(train_loader, leave=True)
        for batch in loop:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            loop.set_description(f"Epoch {epoch+1}")
            loop.set_postfix(loss=loss.item())

        # Validation Step
        model.eval()
        with torch.no_grad():
            val_loss = 0
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()
        print(f"Validation Loss after Epoch {epoch+1}: {val_loss/len(val_loader):.4f}")
        model.train()

# Train the Model
train_model(model, train_loader, val_loader, epochs=3)

# Save Model
model.save_pretrained("bug_fixing_t5_model")
tokenizer.save_pretrained("bug_fixing_t5_model")

# Load trained model for inference
def load_model():
    model = T5ForConditionalGeneration.from_pretrained("bug_fixing_t5_model").to(device)
    tokenizer = RobertaTokenizer.from_pretrained("bug_fixing_t5_model")  # FIXED
    return model, tokenizer

model, tokenizer = load_model()

# Code Fixing Function
def fix_code(buggy_code):
    """Generate a fixed version of the buggy code"""
    input_text = "fix: " + buggy_code
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)
    output_ids = model.generate(input_ids, max_length=512)
    fixed_code = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return fixed_code

# Evaluation Function
def evaluate_model(val_buggy, val_fixed):
    predictions = [fix_code(code) for code in val_buggy]

    precision, recall, f1, _ = precision_recall_fscore_support(
        [fixed.split() for fixed in val_fixed],
        [pred.split() for pred in predictions],
        average='macro'
    )

    bleu_scores = [sentence_bleu([fixed.split()], pred.split()) for fixed, pred in zip(val_fixed, predictions)]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, BLEU Score: {avg_bleu:.4f}")

evaluate_model(val_buggy, val_fixed)

# Streamlit UI
if __name__ == "__main__":
    st.title("Bug Detection & Auto Fixing System")
    user_input = st.text_area("Enter buggy code:")
    if st.button("Fix Code"):
        if user_input.strip():
            fixed_output = fix_code(user_input)
            st.subheader("Fixed Code:")
            st.code(fixed_output, language='python')
        else:
            st.warning("Please enter some code to fix.")

!pip install nltk
import nltk
nltk.download('punkt')

# ... [Your existing code] ...

from sklearn.preprocessing import MultiLabelBinarizer

def evaluate_model(val_buggy, val_fixed):
    predictions = [fix_code(code) for code in val_buggy]

    # Prepare data for precision, recall, and F1-score calculation
    all_tokens = set()
    for fixed, pred in zip(val_fixed, predictions):
        all_tokens.update(fixed.split())
        all_tokens.update(pred.split())

    mlb = MultiLabelBinarizer(classes=list(all_tokens))

    # Calculate precision, recall, and F1-score
    precision, recall, f1, _ = precision_recall_fscore_support(
        mlb.fit_transform([fixed.split() for fixed in val_fixed]),
        mlb.transform([pred.split() for pred in predictions]),
        average='macro'
    )

    # Calculate BLEU score
    bleu_scores = [sentence_bleu([fixed.split()], pred.split()) for fixed, pred in zip(val_fixed, predictions)]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, BLEU Score: {avg_bleu:.4f}")

# ... [Rest of your code] ...

import pandas as pd
import torch
!pip install streamlit
import streamlit as st
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.metrics import precision_recall_fscore_support
from nltk.translate.bleu_score import sentence_bleu

# Import Transformer Model and Tokenizer
from transformers import RobertaTokenizer, T5ForConditionalGeneration

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load dataset
try:
    df = pd.read_csv("/content/synthetic_bug_dataset_1000_final_refreshed.csv").dropna()
except FileNotFoundError:
    print("Error: File not found. Please provide the correct file path.")
    raise  # Stop execution if file is not found

def preprocess_data(df):
    if df.empty:
        print("DataFrame is empty. Please load the dataset first.")
        return [], []

    # Find column names containing 'buggy' and 'fixed'
    buggy_column_name = df.columns[df.columns.str.contains('buggy', case=False)][0]
    fixed_column_name = df.columns[df.columns.str.contains('fixed', case=False)][0]

    return df[buggy_column_name].tolist(), df[fixed_column_name].tolist()

# Load Pretrained Model
MODEL_NAME = "Salesforce/codet5-small"
tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)

class CodeDataset(Dataset):
    def __init__(self, buggy, fixed, tokenizer, max_length=512):
        self.buggy = buggy
        self.fixed = fixed
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.buggy)

    def __getitem__(self, idx):
        buggy_code = "fix: " + self.buggy[idx]
        fixed_code = self.fixed[idx]

        source = self.tokenizer(buggy_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")
        target = self.tokenizer(fixed_code, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")

        return {
            "input_ids": source["input_ids"].squeeze(),
            "attention_mask": source["attention_mask"].squeeze(),
            "labels": target["input_ids"].squeeze()
        }

# Train-Test Split
buggy, fixed = preprocess_data(df)
train_buggy, val_buggy, train_fixed, val_fixed = train_test_split(buggy, fixed, test_size=0.2, random_state=42)

# Create Datasets & Dataloaders
train_dataset = CodeDataset(train_buggy, train_fixed, tokenizer)
val_dataset = CodeDataset(val_buggy, val_fixed, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4)
# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

import torch
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training Function
def train_model(model, train_loader, val_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        loop = tqdm(train_loader, leave=True)
        for batch in loop:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            loop.set_description(f"Epoch {epoch+1}")
            loop.set_postfix(loss=loss.item())

        # Validation Step
        model.eval()
        with torch.no_grad():
            val_loss = 0
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
           attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()
        print(f"Validation Loss after Epoch {epoch+1}: {val_loss/len(val_loader):.4f}")
        model.train()
train_model(model, train_loader, val_loader, epochs=3)

# Save Model
model.save_pretrained("bug_fixing_t5_model")
tokenizer.save_pretrained("bug_fixing_t5_model")
model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5-small").to(device)

# Load trained model for inference
def load_model():
    model = T5ForConditionalGeneration.from_pretrained("bug_fixing_t5_model").to(device)
    tokenizer = RobertaTokenizer.from_pretrained("bug_fixing_t5_model")  # FIXED
    return model, tokenizer

model, tokenizer = load_model()

def evaluate_model(val_buggy, val_fixed):
    predictions = [fix_code(code) for code in val_buggy]

    precision, recall, f1, _ = precision_recall_fscore_support(
        [fixed.split() for fixed in val_fixed],
        [pred.split() for pred in predictions],
        average='macro'
    )

    bleu_scores = [sentence_bleu([fixed.split()], pred.split()) for fixed, pred in zip(val_fixed, predictions)]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, BLEU Score: {avg_bleu:.4f}")

evaluate_model(val_buggy, val_fixed)

!pip install streamlit
# Streamlit UI
if __name__ == "__main__":
    st.title("Bug Detection & Auto Fixing System")
    user_input = st.text_area("Enter buggy code:")
    if st.button("Fix Code"):
        if user_input.strip():
            fixed_output = fix_code(user_input)
            st.subheader("Fixed Code:")
            st.code(fixed_output, language='python')
        else:
            st.warning("Please enter some code to fix.")

